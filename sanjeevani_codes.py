# -*- coding: utf-8 -*-
"""Sanjeevani_codes

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_iEh-z7xrKVAMhGC_D_f94cR07gzOMqP

THIS  PROJECT IS

Innitial Setup
===========================
"""

!pip install fastapi uvicorn pyngrok firebase-admin nest-asyncio

!pip install fastapi uvicorn firebase-admin

from google.colab import drive
drive.mount('/content/drive')

from google.colab import files
uploaded = files.upload()

!ps aux | grep uvicorn

!npm install -g firebase-tools

!firebase login --no-localhost

from google.colab import drive
drive.mount('/content/drive')

"""Fire base
====================

"""

!pip install firebase-admin

import firebase_admin
from firebase_admin import credentials, firestore

# Check if Firebase is already initialized
if not firebase_admin._apps:
    cred = credentials.Certificate("/content/sanjeevani-d6b23-firebase-adminsdk-fbsvc-03180879d4.json")  # Ensure correct filename
    firebase_admin.initialize_app(cred)

db = firestore.client()

print("âœ… Firebase connected successfully!")

from firebase_admin import firestore

db = firestore.client()

# Example: Storing patient data
patient_ref = db.collection("patients").document("patient_001")
patient_ref.set({
    "name": "John Doe",
    "age": 30,
    "condition": "Diabetes",
    "last_visit": "2025-03-10"
})

print("Patient data added successfully!")

doc = db.collection("patients").document("patient_001").get()
if doc.exists:
    print("ðŸ“„ Retrieved Data:", doc.to_dict())
else:
    print("âŒ No such document!")

doc_ref = db.collection("patients").document("test_patient")
doc_ref.set({
    "name": "Arman",
    "age": 20,
    "condition": "Healthy"
})
print("âœ… Data written successfully!")

doc = db.collection("patients").document("test_patient").get()
if doc.exists:
    print("ðŸ“„ Retrieved Data:", doc.to_dict())
else:
    print("âŒ No such document!")

doc = db.collection("patients").document("patient_001").get()
if doc.exists:
    print("Patient Data:", doc.to_dict())
else:
    print("No data found!")

"""**Code for CRUD Operation**"""

from firebase_admin import firestore

db = firestore.client()

# Create (Add new patient)
def add_patient(patient_id, name, age, condition, last_visit):
    patient_ref = db.collection("patients").document(patient_id)
    patient_ref.set({
        "name": name,
        "age": age,
        "condition": condition,
        "last_visit": last_visit
    })
    print(f"âœ… Patient {name} added successfully!")

# Read (Fetch patient details)
def get_patient(patient_id):
    patient_ref = db.collection("patients").document(patient_id)
    doc = patient_ref.get()
    if doc.exists:
        print(f"ðŸ” Retrieved Data: {doc.to_dict()}")
    else:
        print("âŒ No patient found!")

# Update (Modify existing patient data)
def update_patient(patient_id, update_data):
    patient_ref = db.collection("patients").document(patient_id)
    patient_ref.update(update_data)
    print(f"âœï¸ Patient {patient_id} updated successfully!")

# Delete (Remove patient data)
def delete_patient(patient_id):
    patient_ref = db.collection("patients").document(patient_id)
    patient_ref.delete()
    print(f"ðŸ—‘ï¸ Patient {patient_id} deleted successfully!")

# Example usage
add_patient("patient_001", "John Doe", 30, "Diabetes", "2025-03-10")
get_patient("patient_001")
update_patient("patient_001", {"age": 31, "last_visit": "2025-03-15"})
delete_patient("patient_001")

"""Code to Add a Doctor

"""

from firebase_admin import firestore

db = firestore.client()

# Add doctor details
doctor_ref = db.collection("doctors").document("doctor_001")
doctor_ref.set({
    "name": "Dr. Aakash Mehta",
    "specialization": "Cardiology",
    "experience": 12,
    "contact": "+91 9876543210",
    "available": True
})

print("Doctor added successfully! âœ…")

# Retrieve doctor data
doctor_data = doctor_ref.get().to_dict()
print("ðŸ” Retrieved Data:", doctor_data)

"""Code to Add an Appointment"""



from firebase_admin import firestore

db = firestore.client()

# Add an appointment
appointment_ref = db.collection("appointments").document("appointment_001")
appointment_ref.set({
    "patient_id": "patient_001",
    "doctor_id": "doctor_001",
    "date": "2025-03-20",
    "time": "10:30 AM",
    "status": "Scheduled"
})

print("ðŸ“… Appointment added successfully! âœ…")

# Retrieve appointment details
appointment_data = appointment_ref.get().to_dict()
print("ðŸ” Retrieved Appointment:", appointment_data)

"""Store Doctor Information"""

doctor_ref = db.collection("doctors").document("doctor_001")
doctor_ref.set({
    "name": "Dr. Smith",
    "specialization": "Cardiologist",
    "experience": 10,
    "contact": "+91 9876543210"
})

print("ðŸ©º Doctor added successfully!")

doc_data = doctor_ref.get().to_dict()
print("ðŸ” Retrieved Doctor:", doc_data)

"""Store & Fetch Medical Records"""

record_ref = db.collection("medical_records").document("record_001")
record_ref.set({
    "patient_id": "patient_001",
    "doctor_id": "doctor_001",
    "diagnosis": "Hypertension",
    "prescription": "Medicine XYZ - 2 times a day",
    "date": "2025-03-15"
})

print("ðŸ“œ Medical Record stored successfully!")

# Retrieve the record
record_data = record_ref.get().to_dict()
print("ðŸ” Retrieved Record:", record_data)

patients = db.collection("patients").limit(10).stream()
print("ðŸ‘¨â€âš•ï¸ All Patients:")
for patient in patients:
    print(patient.id, patient.to_dict())

"""Book an Online Consultation (Video Call Link Generation)"""

import random

consult_ref = db.collection("consultations").document("consult_001")
video_link = f"https://meet.example.com/{random.randint(100000, 999999)}"

consult_ref.set({
    "patient_id": "patient_001",
    "doctor_id": "doctor_001",
    "date": "2025-03-22",
    "time": "5:00 PM",
    "video_link": video_link
})

print(f"ðŸ“¹ Online Consultation booked! Link: {video_link}")

"""User Authentication (Sign Up with Email & Password)"""

import firebase_admin
from firebase_admin import credentials, auth

if not firebase_admin._apps:
    cred = credentials.Certificate("/content/sample_data/sanjeevani-d6b23-firebase-adminsdk-fbsvc-03180879d4.json")
    firebase_admin.initialize_app(cred)

import firebase_admin
from firebase_admin import credentials, auth

cred = credentials.Certificate("/content/sanjeevani-d6b23-firebase-adminsdk-fbsvc-03180879d4.json")

try:
    firebase_admin.initialize_app(cred, name="sanjeevani_app")
except ValueError:
    firebase_admin.get_app("sanjeevani_app")  # Use the existing app

users = auth.list_users()
for user in users.users:
    print(f"User ID: {user.uid}, Email: {user.email}")

""" Firebase Authentication (Login/Signup)"""

pip install firebase-admin

import firebase_admin
from firebase_admin import credentials, auth

# Initialize Firebase only once
if not firebase_admin._apps:
    cred = credentials.Certificate("sanjeevani-d6b23-firebase-adminsdk-fbsvc-03180879d4.json")
    firebase_admin.initialize_app(cred)

def create_user(email, password):
    try:
        user = auth.create_user(email=email, password=password)
        print(f"User created: {user.uid}")
    except firebase_admin.auth.EmailAlreadyExistsError:
        print(f"User with email {email} already exists.")

create_user("testuser@gmail.com", "Test@1234")

def get_user(uid):
    user = auth.get_user(uid)
    print(f"User: {user.email}, UID: {user.uid}")

get_user("FSt9NVrkKeSmQe7qmeE5GGd3Z4I3")

import firebase_admin
from firebase_admin import auth, credentials

if not firebase_admin._apps:  # Avoid re-initializing Firebase
    cred = credentials.Certificate("/content/sample_data/sanjeevani-d6b23-firebase-adminsdk-fbsvc-03180879d4.json")
    firebase_admin.initialize_app(cred)

email = "testuser@gmail.com"
password = "YourStrongPassword"

try:
    user = auth.get_user_by_email(email)
    print(f"User with email {email} already exists. UID: {user.uid}")
except firebase_admin.auth.UserNotFoundError:
    user = auth.create_user(email=email, password=password)
    print(f"User created successfully. UID: {user.uid}")

user = auth.get_user_by_email("testuser@gmail.com")
print(f"User UID: {user.uid}, Email: {user.email}")

auth.update_user(
    user.uid,
    display_name="Test User",
    phone_number="+911234567890",
    password="NewSecurePassword"
)
print("User updated successfully!")

auth.delete_user(user.uid)
print("User deleted successfully!")

# Normally, login happens on the frontend, but for backend verification:
def verify_user(id_token):
    decoded_token = auth.verify_id_token(id_token)
    print(f"ðŸ”‘ User Verified: {decoded_token['uid']}")

# You need to pass the Firebase Auth ID token from the frontend

import threading
import time

def listen_updates():
    while True:
        time.sleep(5)  # Keep checking for updates
        print("Listening for updates...")

# Run in a separate thread
thread = threading.Thread(target=listen_updates, daemon=True)
thread.start()

print("âœ… Listening for real-time patient updates in the background!")

import firebase_admin
from firebase_admin import credentials, firestore

# Initialize Firebase if not already initialized
if not firebase_admin._apps:
    cred = credentials.Certificate("path/to/serviceAccountKey.json")
    firebase_admin.initialize_app(cred)

# Firestore client
db = firestore.client()

# Data to add
appointment_data = {
    "date": "2025-03-25",
    "doctor_id": "doctor_002",
    "patient_id": "patient_002",
    "status": "Scheduled",
    "time": "11:00 AM"
}

# Add to Firestore
db.collection("appointments").document("appointment_002").set(appointment_data)

print("Appointment added successfully!")

data = {
    "name": "Dr. Ravi Kumar",
    "specialization": "Cardiologist",
    "hospital": "Apollo Hospital, Mysore",
    "experience": 12
}

db.collection("doctors").add(data)
print("Doctor data added successfully!")

docs = db.collection("doctors").stream()
for doc in docs:
    print(f"{doc.id} => {doc.to_dict()}")

import os
os._exit(0)

!pip install flask flask_cors pyrebase4 gunicorn
!pip install pyngrok

!ngrok authtoken 2uLEizRkpdkP4RecYwx8W17VomB_2z9wA14x6WaFfe1vXJFSh

from flask import Flask

app = Flask(__name__)

@app.route('/')
def home():
    return "Hello, Flask is running!"

if __name__ == '__main__':
    app.run()

from flask import Flask, request, jsonify
from flask_cors import CORS
import pyrebase
from pyngrok import ngrok

# Firebase Configuration
firebase_config = {
    "apiKey": "your_api_key",
    "authDomain": "http://your-project.firebaseapp.com",
    "databaseURL": "https://your_project_id.firebaseio.com",
    "storageBucket": "http://your-project.appspot.com",
}

firebase = pyrebase.initialize_app(firebase_config)
auth = firebase.auth()
db = firebase.database()

app = Flask(__name__)
CORS(app)

@app.route('/signup', methods=['POST'])
def signup():
    data = request.json
    email = data.get("email")
    password = data.get("password")

    try:
        user = auth.create_user_with_email_and_password(email, password)
        return jsonify({"message": "User registered successfully", "user_id": user['localId']}), 200
    except Exception as e:
        return jsonify({"error": str(e)}), 400

@app.route('/login', methods=['POST'])
def login():
    data = request.json
    email = data.get("email")
    password = data.get("password")

    try:
        user = auth.sign_in_with_email_and_password(email, password)
        return jsonify({"message": "Login successful", "user_id": user['localId']}), 200
    except Exception as e:
        return jsonify({"error": str(e)}), 400

if __name__ == '__main__':
    public_url = ngrok.connect(5000).public_url
    print("Public URL:", public_url)
    app.run(port=5000)

"""AI PART
=================

#   phase1

phase 1
"""

import pandas as pd

# Create a dataset
data = {
    "Symptom 1": ["Fever", "Cough", "Headache", "Fatigue", "Skin Rash"],
    "Symptom 2": ["Cough", "Shortness of breath", "Fever", "Muscle Pain", "Itching"],
    "Symptom 3": ["Sore Throat", "Fatigue", "Nausea", "Dizziness", "Red Spots"],
    "Disease": ["Flu", "Pneumonia", "Migraine", "COVID-19", "Chickenpox"]
}

df = pd.DataFrame(data)

# Save dataset as CSV
df.to_csv("disease_dataset.csv", index=False)

print("âœ… Dataset created successfully!")

df = pd.read_csv("disease_dataset.csv")
df.head()

from google.colab import files
uploaded = files.upload()

import numpy as np
import pandas as pd
import joblib
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder

# Load dataset (replace with your dataset file)
df = pd.read_csv("disease_dataset.csv")

# Combine all symptoms into one text column
df["symptom_text"] = df.iloc[:, :-1].apply(lambda x: " ".join(x.dropna()), axis=1)

# Encode diseases into numerical labels
label_encoder = LabelEncoder()
df["Disease_Label"] = label_encoder.fit_transform(df["Disease"])

# Convert symptom text into numerical features
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(df["symptom_text"])
y = df["Disease_Label"]

# Train Decision Tree Classifier
model = DecisionTreeClassifier()
model.fit(X, y)

# Save the trained model and encoders
joblib.dump(model, "disease_model.pkl")
joblib.dump(vectorizer, "symptom_vectorizer.pkl")
joblib.dump(label_encoder, "disease_label_encoder.pkl")

print("Model trained and saved successfully!")

import joblib

# Train your model again (if needed)
# model.fit(X_train, y_train)

# Save the trained model
joblib.dump(model, "disease_prediction_model.pkl")
joblib.dump(label_encoder, "label_encoder.pkl")

print("âœ… Model trained and saved successfully!")

import joblib

# Load the saved model
model = joblib.load("disease_prediction_model.pkl")
label_encoder = joblib.load("label_encoder.pkl")

print("âœ… Model loaded successfully!")

# Load dataset
df = pd.read_csv("disease_dataset.csv")

# Remove non-symptom columns like 'Disease'
all_symptoms = [col for col in df.columns if col.lower() not in ["disease"]]

print("Total extracted symptoms:", len(all_symptoms))  # Should be 18

import pandas as pd

# Load dataset
df = pd.read_csv("disease_dataset.csv")

# Check column names
print("Columns in dataset:", df.columns)

# Identify symptom columns (excluding 'Disease' or others)
possible_non_symptom_columns = ["Disease", "Symptoms"]  # Adjust based on dataset
all_symptoms = [col for col in df.columns if col not in possible_non_symptom_columns]

print("Total extracted symptoms:", len(all_symptoms))  # Should be 18
print("Extracted symptom columns:", all_symptoms)

import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer

# Load dataset
df = pd.read_csv("disease_dataset.csv")

# Convert symptoms into a list format
df["Symptoms"] = df[["Symptom 1", "Symptom 2", "Symptom 3"]].values.tolist()

# One-hot encode symptoms
mlb = MultiLabelBinarizer()
X = pd.DataFrame(mlb.fit_transform(df["Symptoms"]), columns=mlb.classes_)

# Add disease labels
y = df["Disease"]

print("New feature matrix shape:", X.shape)  # Should match model expectations

import numpy as np

# Define the complete set of symptoms (from your model training)
all_symptoms = ["Symptom A", "Symptom B", "Symptom C", ..., "Symptom R"]  # 18 symptoms

# Update feature matrix to ensure all 18 symptoms exist
for symptom in all_symptoms:
    if symptom not in X.columns:
        X[symptom] = 0  # Add missing symptoms with 0 value

# Ensure correct column order
X = X[all_symptoms]

print("Final feature matrix shape:", X.shape)  # Should now be (5, 18)

import pandas as pd

# Define the full list of symptoms from your trained model
all_symptoms = ["Symptom 1", "Symptom 2", "Symptom 3", ..., "Symptom 18"]  # Replace with actual symptoms

# Load dataset
df = pd.read_csv("disease_dataset.csv")

# Fill missing symptoms with 0
for symptom in all_symptoms:
    if symptom not in df.columns:
        df[symptom] = 0  # Add missing symptoms with 0 values

# Keep only the required symptoms
X = df[all_symptoms]

print("âœ… Final feature matrix shape:", X.shape)  # Should be (5, 18)

import pandas as pd

# Define all expected symptoms from your trained model
expected_symptoms = ["Symptom 1", "Symptom 2", "Symptom 3", ..., "Symptom 18"]  # Replace with actual names

# Load dataset
df = pd.read_csv("disease_dataset.csv")

# Add missing symptom columns
for symptom in expected_symptoms:
    if symptom not in df.columns:
        df[symptom] = 0  # Add missing symptoms with default 0

# Keep only the required symptoms
X = df[expected_symptoms]

print("âœ… Corrected feature matrix shape:", X.shape)  # Must be (5, 18)

print("Columns in dataset:", df.columns)

print("Columns in dataset:", df.columns.tolist())

print(df.columns.tolist())  # This will show if 'Ellipsis' is a column.

# Remove Ellipsis from column names
df = df.loc[:, ~df.columns.astype(str).str.contains("Ellipsis")]

# Ensure all 18 symptoms exist
expected_symptoms = [f"Symptom {i}" for i in range(1, 19)]
for symptom in expected_symptoms:
    if symptom not in df.columns:
        df[symptom] = 0  # Fill missing symptom columns with 0

# Extract corrected feature matrix
X = df[expected_symptoms]
print("âœ… Final feature matrix shape:", X.shape)  # Should be (rows, 18)

expected_symptoms = [f"Symptom {i}" for i in range(1, 19)]
X = df[expected_symptoms]
print("âœ… Final feature matrix shape:", X.shape)  # Must be (5,18)

# Load the saved model and label encoder
model = joblib.load("disease_prediction_model.pkl")
label_encoder = joblib.load("label_encoder.pkl")

# Example test symptoms
test_symptoms = ["Symptom 1", "Symptom 5", "Symptom 9"]  # Modify as needed

# Convert test symptoms into a binary input vector
input_vector = [1 if symptom in test_symptoms else 0 for symptom in expected_symptoms]

# Make prediction
predicted_label = model.predict([input_vector])[0]
predicted_disease = label_encoder.inverse_transform([predicted_label])[0]

print("ðŸ©º Predicted Disease:", predicted_disease)

"""# phase 2 :fast API"""

!pip install pyngrok
!ngrok authtoken YOUR_NGROK_AUTH_TOKEN

!ngrok config add-authtoken 2uLEizRkpdkP4RecYwx8W17VomB_2z9wA14x6WaFfe1vXJFSh

from pyngrok import ngrok

# Start the tunnel for your Flask app (or any local server)
public_url = ngrok.connect(5000)
print(f"Public URL: {public_url}")

!ls /content/

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/app.py
# from flask import Flask
# 
# app = Flask(__name__)
# 
# @app.route("/")
# def home():
#     return "Hello, Flask is running!"
# 
# if __name__ == "__main__":
#     app.run(host="0.0.0.0", port=5000)
#

!pip install flask-ngrok

!pip install pyngrok
from flask import Flask
from pyngrok import ngrok

app = Flask(__name__)

ngrok.set_auth_token("2uLEizRkpdkP4RecYwx8W17VomB_2z9wA14x6WaFfe1vXJFSh")  # Get token from https://dashboard.ngrok.com
public_url = ngrok.connect(5000).public_url
print("Public URL:", public_url)

@app.route("/")
def home():
    return "Flask is running!"

if __name__ == "__main__":
    app.run(port=5000)

!pip install flask flask-ngrok

from flask import Flask
from flask_ngrok import run_with_ngrok

app = Flask(__name__)
run_with_ngrok(app)  # Start ngrok when running the app

@app.route("/")
def home():
    return "Flask is running!"

app.run()





















!python3 /content/app.py

!kill $(pgrep -f ngrok)
!ngrok http 5000

!python app.py















def predict_disease(symptoms_text):
    # Load saved model and encoders
    model = joblib.load("disease_model.pkl")
    vectorizer = joblib.load("symptom_vectorizer.pkl")
    label_encoder = joblib.load("disease_label_encoder.pkl")

    # Convert input symptoms into feature vector
    symptoms_vector = vectorizer.transform([symptoms_text])

    # Predict disease
    predicted_label = model.predict(symptoms_vector)[0]
    predicted_disease = label_encoder.inverse_transform([predicted_label])[0]

    return predicted_disease

# Test the prediction
user_input = "fever cough sore throat"
predicted_disease = predict_disease(user_input)
print(f"Predicted Disease: {predicted_disease}")

"""Set Up Symptom Checker Backend (FastAPI)"""

pip install flask-cors

!pip install fastapi pydantic firebase-admin uvicorn

import pandas as pd

# Sample medical data
data = [
    ["Fever", "Cough", "Fatigue", "Influenza (Flu)"],
    ["Headache", "Dizziness", "Nausea", "Migraine"],
    ["Joint Pain", "Rash", "Fever", "Dengue"],
    ["Chest Pain", "Shortness of Breath", "Sweating", "Heart Attack"],
    ["Sore Throat", "Runny Nose", "Sneezing", "Common Cold"],
]

# Create DataFrame
df = pd.DataFrame(data, columns=["Symptom 1", "Symptom 2", "Symptom 3", "Disease"])

# Save to CSV
df.to_csv("symptom_disease_data.csv", index=False)

print("Dataset created: symptom_disease_data.csv")

import pandas as pd
import numpy as np
import pickle
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import MultiLabelBinarizer

# Step 1: Creating a Sample Dataset
data = {
    "Disease": ["Flu", "COVID-19", "Malaria", "Dengue", "Typhoid"],
    "Symptoms": [
        ["fever", "cough", "fatigue"],
        ["fever", "cough", "breathlessness"],
        ["fever", "chills", "sweating"],
        ["fever", "rash", "joint pain"],
        ["fever", "headache", "abdominal pain"]
    ]
}

df = pd.DataFrame(data)

# Step 2: Convert Symptoms into Numerical Format
mlb = MultiLabelBinarizer()
X = mlb.fit_transform(df["Symptoms"])  # One-Hot Encoding of Symptoms
y = np.arange(len(df))  # Assigning a unique number for each disease

# Step 3: Train the Model
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Step 4: Save the Model
with open("disease_model.pkl", "wb") as f:
    pickle.dump((model, mlb, df), f)

print("Model trained and saved as 'disease_model.pkl'")

import pandas as pd

# Load dataset
df = pd.read_csv("symptom_disease_data.csv")

# Display first few rows
print(df.head())

from sklearn.preprocessing import MultiLabelBinarizer

# Convert symptoms into lists
df["Symptoms"] = df.iloc[:, :-1].values.tolist()

# One-hot encoding
mlb = MultiLabelBinarizer()
X = mlb.fit_transform(df["Symptoms"])

# Target variable (Disease)
y = df["Disease"]

print(df.head())
print(df.info())  # Check for missing values

print(df["Symptoms"].head())  # Should be a list of symptoms per row

print(df["Disease"].value_counts())  # Check distribution

df["Symptoms"] = df["Symptoms"].apply(lambda x: eval(x) if isinstance(x, str) else x)

from sklearn.preprocessing import MultiLabelBinarizer

mlb = MultiLabelBinarizer()
X = mlb.fit_transform(df["Symptoms"])

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y = le.fit_transform(df["Disease"])

from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder
import pandas as pd

# Convert symptoms from string to list
df["Symptoms"] = df["Symptoms"].apply(lambda x: eval(x) if isinstance(x, str) else x)

# One-hot encode symptoms
mlb = MultiLabelBinarizer()
X = mlb.fit_transform(df["Symptoms"])

# Encode diseases
le = LabelEncoder()
y = le.fit_transform(df["Disease"])

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X, y)

accuracy = model.score(X, y)  # Use all data for evaluation
print(f"Model Accuracy: {accuracy * 100:.2f}%")

sample_symptoms = [["Fever", "Cough", "Fatigue"]]  # Change this to test
sample_encoded = mlb.transform(sample_symptoms)
predicted_disease = le.inverse_transform(model.predict(sample_encoded))
print("Predicted Disease:", predicted_disease[0])

import pandas as pd
import random

# Define symptoms and diseases
diseases = [
    ("Influenza (Flu)", ["Fever", "Cough", "Fatigue"]),
    ("Migraine", ["Headache", "Dizziness", "Nausea"]),
    ("Dengue", ["Joint Pain", "Rash", "Fever"]),
    ("Heart Attack", ["Chest Pain", "Shortness of Breath", "Sweating"]),
    ("Common Cold", ["Sore Throat", "Runny Nose", "Sneezing"]),
    ("Diabetes", ["Frequent Urination", "Increased Thirst", "Fatigue"]),
    ("Hypertension", ["Headache", "Nosebleeds", "Dizziness"]),
    ("Tuberculosis", ["Cough", "Weight Loss", "Night Sweats"]),
    ("Asthma", ["Wheezing", "Shortness of Breath", "Cough"]),
    ("Pneumonia", ["Fever", "Cough", "Chest Pain"])
]

# Generate synthetic dataset
data = []
for _ in range(100):  # Create 100 samples
    disease, symptoms = random.choice(diseases)
    data.append([symptoms[0], symptoms[1], symptoms[2], disease, symptoms])

df = pd.DataFrame(data, columns=["Symptom 1", "Symptom 2", "Symptom 3", "Disease", "Symptoms"])

# Save the dataset
df.to_csv("synthetic_medical_dataset.csv", index=False)
print("Synthetic dataset created successfully!")

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Encoding categorical values
encoder = LabelEncoder()
df["Disease"] = encoder.fit_transform(df["Disease"])

# Encoding symptoms
symptom_encoders = {}
for col in ["Symptom 1", "Symptom 2", "Symptom 3"]:
    symptom_encoders[col] = LabelEncoder()
    df[col] = symptom_encoders[col].fit_transform(df[col])

# Splitting data into train and test sets
X = df[["Symptom 1", "Symptom 2", "Symptom 3"]]
y = df["Disease"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Data Encoding & Splitting Done âœ…")

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Initialize and train the model
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluate accuracy
accuracy = accuracy_score(y_test, y_pred) * 100
print(f"Model Accuracy: {accuracy:.2f}%")

all_symptoms = sorted(set(df["Symptom 1"]) | set(df["Symptom 2"]) | set(df["Symptom 3"]))

import pandas as pd
from sklearn.tree import DecisionTreeClassifier

# Convert symptoms into one-hot encoding
for symptom in all_symptoms:
    df[symptom] = df.apply(lambda row: int(symptom in row["Symptoms"]), axis=1)

# Prepare training data
X = df[all_symptoms]  # Use all symptoms as features
y = df["Disease"]

# Train model
model = DecisionTreeClassifier()
model.fit(X, y)

def predict_disease(symptom1, symptom2, symptom3):
    input_vector = [1 if symptom in [symptom1, symptom2, symptom3] else 0 for symptom in all_symptoms]
    predicted_disease = model.predict([input_vector])[0]
    return predicted_disease

# Example test
print(predict_disease("Fever", "Cough", "Fatigue"))

print(f"Training Features Shape: {X.shape}")

def predict_disease(symptom1, symptom2, symptom3):
    input_vector = [1 if symptom in [symptom1, symptom2, symptom3] else 0 for symptom in all_symptoms]
    print(f"Input Vector Length: {len(input_vector)}")  # Debugging step
    predicted_disease = model.predict([input_vector])[0]
    return predicted_disease

def predict_disease(symptoms):
    input_vector = [1 if symptom in symptoms else 0 for symptom in all_symptoms]
    if len(input_vector) != 9:
        print(f"Warning: Input vector size {len(input_vector)} does not match expected 9 features.")
    return model.predict([input_vector])[0]

predicted_disease = predict_disease(["Fever", "Cough", "Fatigue"])  # Pass a list
print(f"Predicted Disease: {predicted_disease}")

from sklearn.preprocessing import LabelEncoder

# Initialize and fit label encoder (during training)
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)  # Encode disease labels

# Ensure it's available before predicting
def predict_disease(symptoms):
    input_vector = [1 if symptom in symptoms else 0 for symptom in all_symptoms]
    predicted_label = model.predict([input_vector])[0]
    predicted_disease = label_encoder.inverse_transform([predicted_label])[0]  # Use the trained label_encoder
    return predicted_disease

# Example test case
test_symptoms = ["Fever", "Cough", "Fatigue"]
predicted_disease = predict_disease(test_symptoms)
print(f"Predicted Disease: {predicted_disease}")

def predict_disease(symptoms):
    input_vector = [1 if symptom in symptoms else 0 for symptom in all_symptoms]
    predicted_label = model.predict([input_vector])[0]
    predicted_disease = label_encoder.inverse_transform([predicted_label])[0]
    return predicted_disease

# Example
test_symptoms = ["Fever", "Cough", "Fatigue"]
predicted_disease = predict_disease(test_symptoms)
print(f"Predicted Disease: {predicted_disease}")

# Ensure 'all_symptoms' contains all unique symptoms from the dataset

input_vector = [1 if symptom in test_symptoms else 0 for symptom in all_symptoms]

# Predict the disease
predicted_label = model.predict([input_vector])[0]  # Numerical prediction
predicted_disease = label_encoder.inverse_transform([predicted_label])[0]  # Convert to disease name

print(f"Predicted Disease: {predicted_disease}")

predicted_label = model.predict([input_vector])[0]  # Get numerical prediction
predicted_disease = label_encoder.inverse_transform([predicted_label])[0]  # Convert back to disease name
print(f"Predicted Disease: {predicted_disease}")

from sklearn.linear_model import LogisticRegression

model = LogisticRegression(max_iter=500)
model.fit(X_train, y_train)
accuracy = model.score(X_test, y_test)

print(f"Logistic Regression Accuracy: {accuracy * 100:.2f}%")

print(y_train.value_counts())

print(X_train.head())

model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
model.fit(X_train, y_train)

print(y_train.value_counts())

print(X_train.shape)
print(y_train.shape)

# Function to encode symptoms as binary vector
def encode_symptoms(symptoms, all_symptoms):
    return [1 if symptom in symptoms else 0 for symptom in all_symptoms]

# Test encoding on a sample
sample_symptoms = ['Fever', 'Cough']  # Replace with an actual sample from your dataset
all_symptoms = ['Fever', 'Cough', 'Cold', 'Headache']  # Replace with your all_symptoms list
encoded_sample = encode_symptoms(sample_symptoms, all_symptoms)
print("Encoded Sample:", encoded_sample)

import firebase_admin
from firebase_admin import credentials, firestore

# Check if Firebase is already initialized
if not firebase_admin._apps:
    cred = credentials.Certificate("/content/sample_data/sanjeevani-d6b23-firebase-adminsdk-fbsvc-03180879d4.json")
    firebase_admin.initialize_app(cred)

db = firestore.client()
print("Firebase initialized successfully!")

import firebase_admin
from firebase_admin import credentials, firestore

# Check if Firebase is already initialized
if not firebase_admin._apps:
    cred = credentials.Certificate("/content/sample_data/sanjeevani-d6b23-firebase-adminsdk-fbsvc-03180879d4.json")
    firebase_admin.initialize_app(cred)

db = firestore.client()
print("Firebase initialized successfully!")

import firebase_admin
from firebase_admin import credentials, firestore

# Reference to Firestore
db = firestore.client()

# Example: Adding a patient document
patient_data = {
    "name": "John Doe",
    "age": 30,
    "symptoms": ["fever", "cough"],
    "diagnosis": "Flu"
}

# Add patient to Firestore
db.collection("patients").add(patient_data)

print("Patient data added successfully!")

import pandas as pd

# Replace with your actual file path or data loading method
df = pd.read_csv('/content/symptom_disease_data.csv')

print("Training Features Shape:", X_train.shape)
print("Training Labels Shape:", y_train.shape)
print("Testing Features Shape:", X_test.shape)
print("Testing Labels Shape:", y_test.shape)

import numpy as np

print("Number of unique diseases:", len(np.unique(y_train)))

import numpy as np

X_train = np.array(X_train)  # Convert to NumPy array
y_train = np.array(y_train)  # Convert to NumPy array

print("First row of X_train:", X_train[0])
print("First label of y_train:", y_train[0])

print("Unique values in X_train:\n", np.unique(X_train))

print("Unique labels in y_train:", np.unique(y_train))

import pandas as pd

print(pd.DataFrame(X_train).head())

print("All Symptoms List:", all_symptoms)

print(df.columns)  # Check if symptom names exist

all_symptoms = ['Symptom 1', 'Symptom 2', 'Symptom 3']  # Adjust based on actual symptom columns
print("Corrected Symptoms List:", all_symptoms)

def encode_symptoms(symptoms, all_symptoms):
    # If the symptoms in the dataset are labeled as 'Symptom 1', 'Symptom 2', etc.,
    # make sure to use the correct corresponding symptom names for encoding.
    encoded = [1 if symptom in symptoms else 0 for symptom in all_symptoms]
    return encoded

# Update the list of symptoms to match your dataset columns
test_symptoms = ['Symptom 1', 'Symptom 2']  # Ensure these match actual symptom column names
encoded_test = encode_symptoms(test_symptoms, all_symptoms)
print("Encoded Test Symptoms:", encoded_test)

print(df.columns)  # Ensure these are the actual symptom columns (like 'Symptom 1', 'Symptom 2', etc.)

def encode_all_data(df, all_symptoms):
    encoded_data = df[all_symptoms].apply(lambda row: encode_symptoms(row, all_symptoms), axis=1)
    return encoded_data

# Apply to the entire dataset (excluding the disease column)
X_encoded = encode_all_data(df, all_symptoms)

X = X_encoded  # Features (symptoms)
y = df['Disease']  # Target (disease)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import numpy as np

# Your encode function, ensure it returns proper binary values for each symptom
def encode_symptoms(symptoms, all_symptoms):
    return [1 if symptom in symptoms else 0 for symptom in all_symptoms]

# Apply encoding properly to the dataframe
def encode_all_data(df, all_symptoms):
    encoded_data = df[all_symptoms].apply(lambda row: encode_symptoms(row, all_symptoms), axis=1)
    # Convert list of lists to a numpy array for scikit-learn
    return np.array(encoded_data.tolist())

# Split your data into training and testing sets
X_encoded = encode_all_data(df, all_symptoms)
y = df['Disease']  # Assuming your target variable is 'Disease'

X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)

# Train the model
model = LogisticRegression(max_iter=200)  # Increase max_iter if needed
model.fit(X_train, y_train)

# Evaluate the model
print(f"Training Accuracy: {model.score(X_train, y_train)}")
print(f"Testing Accuracy: {model.score(X_test, y_test)}")

print(X_train[:5])  # Check the first 5 rows of the training data

print(df[all_symptoms].head())  # Check the symptom columns in the original dataframe

def encode_symptoms(symptoms, all_symptoms):
    return [1 if symptoms[col] > 0 else 0 for col in all_symptoms]  # Convert to binary

# Convert the symptom columns to numeric values (if they are not already)
df[all_symptoms] = df[all_symptoms].apply(pd.to_numeric, errors='coerce')

def encode_symptoms(row, all_symptoms):
    return [1 if row[col] == 'Yes' else 0 for col in all_symptoms]

def encode_symptoms(row, all_symptoms):
    return [1 if row[col] > 0 else 0 for col in all_symptoms]

print(df.isna().sum())



# Assuming symptom columns are 'Symptom 1', 'Symptom 2', etc.
def encode_symptoms(row, all_symptoms):
    return [1 if row[col] > 0 else 0 for col in all_symptoms]

# Apply encoding again after filling missing values
encoded_data = df[all_symptoms].apply(lambda row: encode_symptoms(row, all_symptoms), axis=1)
print(encoded_data.head())

# Apply encoding and convert to NumPy array
encoded_data = df[all_symptoms].apply(lambda row: encode_symptoms(row, all_symptoms), axis=1)

# Flatten the list of lists into a NumPy array
X = np.array(encoded_data.tolist())

y = df['Disease']  # Ensure this is the column with your target variable

# Ensure that y_train is a 1D array (if it is a Series, use .values to convert it)
y = y.values

print("X_train shape:", X_train.shape)  # Should be (num_samples, num_features)
print("y_train shape:", y_train.shape)  # Should be (num_samples,)

def encode_symptoms(symptoms, all_symptoms):
    return [1 if symptom in symptoms else 0 for symptom in all_symptoms]

test_symptoms = ["Fever", "Cough"]
encoded_test = encode_symptoms(test_symptoms, all_symptoms)

print("Encoded Test Symptoms:", encoded_test)

def encode_symptoms(row, all_symptoms):
    return [1 if row[col] > 0 else 0 for col in all_symptoms]

df.fillna(0, inplace=True)  # Fills NaN with 0

# Apply encoding to all rows in the dataset
encoded_data = df[all_symptoms].apply(lambda row: encode_symptoms(row, all_symptoms), axis=1)

# Check the encoded data
print(encoded_data.head())

# Check the raw symptom data
print(df[all_symptoms].head())

def encode_symptoms(row, all_symptoms):
    return [1 if row[col] == 'Yes' else 0 for col in all_symptoms]

# Apply the encoding to all rows in the dataset
encoded_data = df[all_symptoms].apply(lambda row: encode_symptoms(row, all_symptoms), axis=1)
print(encoded_data.head())

# Fill NaN values with 0
df[all_symptoms] = df[all_symptoms].fillna(0)

# Check the dataframe after filling NaN values
print(df[all_symptoms].head())

def encode_symptoms(row, all_symptoms):
    return [1 if row[col] > 0 else 0 for col in all_symptoms]

# Apply the encoding to all rows in the dataset
encoded_data = df[all_symptoms].apply(lambda row: encode_symptoms(row, all_symptoms), axis=1)
print(encoded_data.head())  # Check the encoded data

def encode_symptoms(row, all_symptoms):
    return [1 if row[col] == 'Yes' else 0 for col in all_symptoms]

# Apply the encoding to all rows in the dataset
encoded_data = df[all_symptoms].apply(lambda row: encode_symptoms(row, all_symptoms), axis=1)
print(encoded_data.head())  # Check the encoded data

print(df[all_symptoms].head())  # Check the actual symptom data

# Apply the encoding to all rows in the dataset
encoded_data = df[all_symptoms].apply(lambda row: encode_symptoms(row, all_symptoms), axis=1)
print(encoded_data.head())  # Check the encoded data

# Correctly apply encoding and ensure each column represents one symptom
encoded_data = pd.DataFrame(df[all_symptoms].apply(lambda row: encode_symptoms(row, all_symptoms), axis=1).tolist())

# Check if the encoding worked as expected
print(encoded_data.head())

# Ensure all values are numeric
encoded_data = encoded_data.astype(int)

# Check the first few rows
print(encoded_data.head())

X = encoded_data
y = df['Disease']  # Assuming 'Disease' is the target column

# Split into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Check the shapes of the training and testing sets
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)

# Train the model
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(max_iter=1000)  # Increase max_iter if needed
model.fit(X_train, y_train)

# Evaluate the model
train_accuracy = model.score(X_train, y_train)
test_accuracy = model.score(X_test, y_test)

print(f"Training Accuracy: {train_accuracy}")
print(f"Testing Accuracy: {test_accuracy}")

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # Ensure X is defined

# Now split the data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

# Improved dataset
data = {
    "Fever": [1, 0, 1, 0, 1, 1, 0, 1, 0, 1],
    "Cough": [0, 1, 1, 0, 1, 0, 1, 1, 0, 0],
    "Headache": [0, 0, 1, 1, 0, 1, 1, 0, 1, 1],
    "Disease": ["Flu", "Cold", "Flu", "Migraine", "Flu", "Flu", "Cold", "Migraine", "Cold", "Flu"]
}

df = pd.DataFrame(data)

# Features & Labels
X = df.drop(columns=["Disease"])
y = df["Disease"]

# Standardizing Features (Helps Logistic Regression)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split Data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)


# Train Model
model = LogisticRegression(max_iter=1000, solver='liblinear')
model.fit(X_train, y_train)

# Evaluate Model
train_acc = model.score(X_train, y_train)
test_acc = model.score(X_test, y_test)

print(f"Training Accuracy: {train_acc:.2f}")
print(f"Testing Accuracy: {test_acc:.2f}")

model = LogisticRegression(C=1.0, solver='liblinear', max_iter=1000)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

from sklearn.svm import SVC
model = SVC(kernel='linear')
model.fit(X_train, y_train)

from sklearn.model_selection import RandomizedSearchCV
param_dist = {'n_estimators': [50, 100, 200],
              'max_depth': [None, 10, 20, 30],
              'min_samples_split': [2, 5, 10]}
rand_search = RandomizedSearchCV(RandomForestClassifier(), param_dist, n_iter=10, cv=3)
rand_search.fit(X_train, y_train)

from sklearn.model_selection import cross_val_score
cross_val_score(model, X_train, y_train, cv=5)

from sklearn.metrics import classification_report
print(classification_report(y_test, model.predict(X_test)))

from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(random_state=42)
X_train_balanced, y_train_balanced = ros.fit_resample(X_train, y_train)

from sklearn.preprocessing import LabelEncoder
import numpy as np

# Encode string labels into integers
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Now check the distribution
print("Train class distribution:", np.bincount(y_train_encoded))
print("Test class distribution:", np.bincount(y_test_encoded))

from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(random_state=42)
X_train_balanced, y_train_balanced = ros.fit_resample(X_train, y_train)

from collections import Counter
print("Class distribution:", Counter(y_train))

from imblearn.over_sampling import SMOTE
import numpy as np

# Duplicate class 2 to have at least 2 samples
X_train_filtered = np.vstack([X_train_filtered, X_train_filtered[y_train_encoded == 2]])
y_train_encoded = np.hstack([y_train_encoded, [2]])

# Apply SMOTE
smote = SMOTE(random_state=42, k_neighbors=1)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train_filtered, y_train_encoded)

print("After SMOTE:", Counter(y_train_balanced))

from sklearn.preprocessing import LabelEncoder

# Encode labels if not done earlier
label_encoder = LabelEncoder()
y_test_encoded = label_encoder.fit_transform(y_test)  # Ensure same encoding

# Train and evaluate
model_rf_balanced = RandomForestClassifier(random_state=42)
model_rf_balanced.fit(X_train_balanced, y_train_balanced)
test_accuracy_balanced = model_rf_balanced.score(X_test, y_test_encoded)
print("Test Accuracy after SMOTE:", test_accuracy_balanced)

model_rf_balanced = RandomForestClassifier(random_state=42)
model_rf_balanced.fit(X_train_balanced, y_train_balanced)

# Evaluate on test data
test_accuracy_balanced = model_rf_balanced.score(X_test, y_test)
print("Test Accuracy after SMOTE:", test_accuracy_balanced)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# Define parameter grid for tuning
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

# Initialize the RandomForest model
model_rf = RandomForestClassifier()

# Perform grid search with cross-validation
grid_search = GridSearchCV(estimator=model_rf, param_grid=param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Best parameters and performance
print("Best parameters:", grid_search.best_params_)
print("Best training score:", grid_search.best_score_)

# Evaluate on test data with the best model
best_model_rf = grid_search.best_estimator_
test_accuracy_rf = best_model_rf.score(X_test, y_test)
print("Test Accuracy with Best Model:", test_accuracy_rf)

print(y_train.value_counts())

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(class_weight='balanced')
model.fit(X_train, y_train)

model_rf = RandomForestClassifier(n_estimators=200, max_depth=5, min_samples_split=2, random_state=42)

model_rf = RandomForestClassifier(n_estimators=300, max_depth=3, min_samples_split=4, random_state=42)

from sklearn.model_selection import StratifiedKFold, cross_val_score

# Stratified K-Fold for better class distribution
skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
cv_scores = cross_val_score(model_rf, X_scaled, y, cv=skf)

print(f"Updated Cross-validation Accuracy: {cv_scores.mean():.2f}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE

# Sample dataset
data = {
    "Fever": [1, 0, 1, 0, 1, 1, 0, 1, 0, 1],
    "Cough": [0, 1, 1, 0, 1, 0, 1, 1, 0, 0],
    "Headache": [0, 0, 1, 1, 0, 1, 1, 0, 1, 1],
    "Disease": ["Flu", "Cold", "Flu", "Migraine", "Flu", "Flu", "Cold", "Migraine", "Cold", "Flu"]
}

df = pd.DataFrame(data)

# Features & Labels
X = df.drop(columns=["Disease"])
y = df["Disease"]

# Standardizing Features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Handle class imbalance using SMOTE with fewer neighbors
smote = SMOTE(random_state=42, k_neighbors=1)
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)

# Split Data
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, stratify=y_resampled, random_state=42)

# Train Model
model_rf = RandomForestClassifier(n_estimators=100, random_state=42)
model_rf.fit(X_train, y_train)

# Evaluate Model
train_acc = model_rf.score(X_train, y_train)
test_acc = model_rf.score(X_test, y_test)
cv_acc = np.mean(cross_val_score(model_rf, X_resampled, y_resampled, cv=3))

print(f"Training Accuracy: {train_acc:.2f}")
print(f"Testing Accuracy: {test_acc:.2f}")
print(f"Cross-validation Accuracy: {cv_acc:.2f}")

from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize model
rf = RandomForestClassifier(random_state=42)

# Perform Grid Search
grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train, y_train)


# Get best parameters
print("Best Parameters:", grid_search.best_params_)

# Train the model with best parameters
best_rf = RandomForestClassifier(**grid_search.best_params_, random_state=42)
best_rf.fit(X_train, y_train)

# Evaluate
train_acc = best_rf.score(X_train, y_train)
test_acc = best_rf.score(X_test, y_test)

print(f"Updated Training Accuracy: {train_acc:.2f}")
print(f"Updated Testing Accuracy: {test_acc:.2f}")

import matplotlib.pyplot as plt
import numpy as np

# Get feature importance scores
importances = grid_search.best_estimator_.feature_importances_
indices = np.argsort(importances)[::-1]  # Sort in descending order

# Plot feature importance
plt.figure(figsize=(8, 5))
plt.title("Feature Importance")
plt.bar(range(X_train.shape[1]), importances[indices], align="center")
plt.xticks(range(X_train.shape[1]), np.array(X.columns)[indices], rotation=45)
plt.xlabel("Symptoms")
plt.ylabel("Importance Score")
plt.show()

import pickle

# Save the trained model
with open("model.pkl", "wb") as f:
    pickle.dump(model, f)

print("Model saved successfully!")

import pickle
import numpy as np
from flask import Flask, request, jsonify

# Load the trained model
with open("model.pkl", "rb") as f:
    model = pickle.load(f)

app = Flask(__name__)

@app.route("/predict", methods=["POST"])
def predict():
    data = request.json
    symptoms = np.array(data["symptoms"]).reshape(1, -1)
    prediction = model.predict(symptoms)[0]
    return jsonify({"disease_prediction": prediction})

if __name__ == "__main__":
    app.run(debug=True)



from sklearn.ensemble import RandomForestClassifier

# Initialize the RandomForest model
model_rf = RandomForestClassifier(n_estimators=100)

# Train the model
model_rf.fit(X_train, y_train)

# Evaluate the model
train_accuracy_rf = model_rf.score(X_train, y_train)
test_accuracy_rf = model_rf.score(X_test, y_test)

print("Random Forest Training Accuracy:", train_accuracy_rf)
print("Random Forest Testing Accuracy:", test_accuracy_rf)

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, y_train)

print("Training Accuracy:", model.score(X_train, y_train))
print("Testing Accuracy:", model.score(X_test, y_test))

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(encoded_data, df['Disease'], test_size=0.2, random_state=42)

# Train the model
model = LogisticRegression()
model.fit(X_train, y_train)

# Evaluate the model
train_accuracy = model.score(X_train, y_train)
test_accuracy = model.score(X_test, y_test)

print(f"Training Accuracy: {train_accuracy}")
print(f"Testing Accuracy: {test_accuracy}")

def encode_symptoms(symptoms, all_symptoms):
    # Create a binary list where '1' indicates presence and '0' indicates absence
    return [1 if symptom in symptoms else 0 for symptom in all_symptoms]

# Test with a sample row
test_symptoms = ["Fever", "Cough"]  # Adjust based on actual symptoms
encoded_test = encode_symptoms(test_symptoms, all_symptoms)
print("Encoded Test Symptoms:", encoded_test)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train your model (example: Logistic Regression)
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)

# Evaluate the model
print(f"Training Accuracy: {model.score(X_train, y_train)}")
print(f"Testing Accuracy: {model.score(X_test, y_test)}")

print("Unique labels in y_train:", np.unique(y_train))
print("Label distribution:\n", pd.Series(y_train).value_counts())

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

accuracy = model.score(X_test, y_test)
print(f"New Model Accuracy: {accuracy * 100:.2f}%")

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

accuracy = model.score(X_test, y_test)
print(f"New Model Accuracy: {accuracy * 100:.2f}%")

from sklearn.ensemble import RandomForestClassifier

# Train a Random Forest model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Evaluate again
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"New Model Accuracy: {accuracy * 100:.2f}%")

from sklearn.metrics import accuracy_score, classification_report

# Get predictions on the test set
y_pred = model.predict(X_test)

# Evaluate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy * 100:.2f}%")

# Detailed performance report
print("Classification Report:\n", classification_report(y_test, y_pred))



pip install --upgrade google-auth

pip install google-auth google-auth-oauthlib google-auth-httplib2 requests

import os
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/content/synthetic_medical_dataset.csv"

import firebase_admin
from firebase_admin import credentials, firestore

# Check if Firebase is already initialized
if not firebase_admin._apps:
    cred = credentials.Certificate("/content/sample_data/sanjeevani-d6b23-firebase-adminsdk-fbsvc-03180879d4.json")
    firebase_admin.initialize_app(cred)

# Get Firestore reference
db = firestore.client()

def encode_symptoms(row, all_symptoms):
    return [1 if row[col] > 0 else 0 for col in all_symptoms]

encoded_data = df[all_symptoms].apply(lambda row: encode_symptoms(row, all_symptoms), axis=1)
X = np.array(encoded_data.tolist())  # Flatten the list of lists into a NumPy array

all_symptoms = ['Symptom 1', 'Symptom 2', 'Symptom 3']  # Adjust based on actual symptom columns
print("Corrected Symptoms List:", all_symptoms)

def encode_symptoms(symptoms, all_symptoms):
    return [1 if symptom in symptoms else 0 for symptom in all_symptoms]

test_symptoms = ["Fever", "Cough"]
encoded_test = encode_symptoms(test_symptoms, all_symptoms)

print("Encoded Test Symptoms:", encoded_test)

import joblib

# Load the trained AI model
model = joblib.load('/content/disease_model.pkl')

# Endpoint to make predictions
@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json()  # Input data from the user
    prediction = model.predict([data['input_features']])
    return jsonify({'prediction': prediction.tolist()})



pip install google-auth google-auth-oauthlib google-auth-httplib2 google-auth-transport-requests grpcio

from google.oauth2 import service_account

credentials = service_account.Credentials.from_service_account_file("/content/sanjeevani-d6b23-firebase-adminsdk-fbsvc-03180879d4.json")

def encode_symptoms(row, all_symptoms):
    return [1 if row[col] > 0 else 0 for col in all_symptoms]













from fastapi import FastAPI
from pydantic import BaseModel
import pickle
import firebase_admin
from firebase_admin import credentials, firestore

# Load Firebase
cred = credentials.Certificate("/content/sample_data/sanjeevani-d6b23-firebase-adminsdk-fbsvc-03180879d4.json")  # Replace with your Firebase key
firebase_admin.initialize_app(cred)
db = firestore.client()

# Load Pre-trained Model
with open("disease_model.pkl", "rb") as f:
    model = pickle.load(f)

app = FastAPI()

# Define Input Format
class SymptomInput(BaseModel):
    symptoms: list

@app.post("/predict")
async def predict_disease(input_data: SymptomInput):
    symptoms = input_data.symptoms
    prediction = model.predict([symptoms])  # Predict disease
    disease = prediction[0]

    # Store query in Firebase
    db.collection("queries").add({"symptoms": symptoms, "prediction": disease})

    return {"disease": disease, "status": "success"}

@app.get("/history")
async def get_history():
    docs = db.collection("queries").stream()
    history = [{"symptoms": doc.to_dict()["symptoms"], "disease": doc.to_dict()["prediction"]} for doc in docs]
    return {"history": history}

"""FASTAPI

"""

!pip install firebase-admin

import firebase_admin
from firebase_admin import credentials, firestore

# Check if Firebase is already initialized
if not firebase_admin._apps:
    cred = credentials.Certificate("/content/sample_data/sanjeevani-d6b23-firebase-adminsdk-fbsvc-03180879d4.json")
    firebase_admin.initialize_app(cred)

# Get Firestore reference
db = firestore.client()

!pip install fastapi uvicorn pyngrok nest-asyncio

!ngrok authtoken 2uLEizRkpdkP4RecYwx8W17VomB_2z9wA14x6WaFfe1vXJFSh

!pip install pyngrok
from pyngrok import ngrok

!ngrok.disconnect(public_url)
public_url = ngrok.connect(8000)
print(f"Public URL: {public_url}")

!ngrok.disconnect(public_url)  # Only if public_url exists
public_url = ngrok.connect(8000)
print(f"Public URL: {public_url}")

!pip install fastapi uvicorn pyngrok

from fastapi import FastAPI
from pyngrok import ngrok
import nest_asyncio
import uvicorn

# Apply nest_asyncio to avoid event loop issues in Colab
nest_asyncio.apply()

app = FastAPI()

@app.get("/")
def read_root():
    return {"message": "Hello, FastAPI from Colab!"}

# Set ngrok authentication and expose port
ngrok.set_auth_token("2uLEizRkpdkP4RecYwx8W17VomB_2z9wA14x6WaFfe1vXJFSh")  # Replace with your token
public_url = ngrok.connect(8000)
print(f"Public URL: {public_url}")

# Start the FastAPI server
uvicorn.run(app, host="0.0.0.0", port=8000)

!pip install fastapi uvicorn pyngrok

from fastapi import FastAPI

app = FastAPI()

@app.get("/")
def read_root():
    return {"message": "Hello, FastAPI from Colab!"}

@app.get("/health")
def health_check():
    return {"status": "OK"}

@app.get("/user/{user_id}")
def get_user(user_id: int):
    return {"user_id": user_id, "message": "User data retrieved"}

@app.post("/create")
def create_item(item: dict):
    return {"message": "Item created", "item": item}

# Commented out IPython magic to ensure Python compatibility.
# %%writefile fastapi_firestore.py
# from fastapi import FastAPI
# from firebase_admin import credentials, firestore, initialize_app
# 
# # Initialize FastAPI app
# app = FastAPI()
# 
# # Firebase Admin SDK Initialization
# cred = credentials.Certificate("sanjeevani-d6b23-firebase-adminsdk-fbsvc-03180879d4.json")
# try:
#     initialize_app(cred)
# except ValueError:
#     pass  # Avoid reinitialization error
# 
# db = firestore.client()
# collection_name = "patients"
# 
# # API Endpoint to add patient data
# @app.post("/add_patient/")
# async def add_patient(data: dict):
#     doc_ref = db.collection(collection_name).add(data)
#     return {"message": "Patient added successfully!"}
# 
# # API Endpoint to fetch patient data
# @app.get("/get_patient/{patient_id}")
# async def get_patient(patient_id: str):
#     doc_ref = db.collection(collection_name).document(patient_id).get()
#     if doc_ref.exists:
#         return doc_ref.to_dict()
#     return {"error": "Patient not found"}
#

from google.colab import files
files.upload()

import os
print(os.listdir())

import firebase_admin
from firebase_admin import credentials

cred = credentials.Certificate("/content/sanjeevani-d6b23-firebase-adminsdk-fbsvc-03180879d4.json")
firebase_admin.initialize_app(cred)

!uvicorn fastapi_firestore:app --reload

!pip install uvicorn

!ls /content

!ls /content

# Commented out IPython magic to ensure Python compatibility.
# %%writefile fastapi_firestore.py
# from fastapi import FastAPI
# 
# app = FastAPI()
# 
# @app.get("/")
# def read_root():
#     return {"message": "Hello, FastAPI!"}
#

!ls /content

!pip install fastapi uvicorn firebase-admin

!uvicorn fastapi_firestore:app --reload

import requests
response = requests.get("http://127.0.0.1:8000")
print(response.json())





"""FRONTED
======================
"""

!pip install flask flask-ngrok pyrebase4 flask-cors

from flask import Flask, render_template, request, jsonify, send_from_directory
from flask_ngrok import run_with_ngrok
from flask_cors import CORS
import os
import pyrebase

# Initialize Flask app
app = Flask(__name__, static_folder="AI Healthcare", template_folder="AI Healthcare")
run_with_ngrok(app)
CORS(app)  # Allows frontend to communicate with backend

# Firebase Configuration
firebaseConfig = {
    "apiKey": "your-api-key",
    "authDomain": "your-project.firebaseapp.com",
    "databaseURL": "https://your-project.firebaseio.com",
    "projectId": "your-project-id",
    "storageBucket": "your-project.appspot.com",
    "messagingSenderId": "your-messaging-sender-id",
    "appId": "your-app-id"
}

# Initialize Firebase
firebase = pyrebase.initialize_app(firebaseConfig)
db = firebase.database()

# Serve HTML pages
@app.route('/')
def home():
    return render_template("index.html")

@app.route('/<page>')
def serve_page(page):
    return render_template(page)

# API to book an appointment (POST Request)
@app.route('/book_appointment', methods=['POST'])
def book_appointment():
    data = request.json  # Get data from frontend
    db.child("appointments").push(data)  # Store in Firebase
    return jsonify({"message": "Appointment booked successfully"})

# API to get booked appointments
@app.route('/get_appointments', methods=['GET'])
def get_appointments():
    appointments = db.child("appointments").get().val()
    return jsonify(appointments)

# File Upload Route (if required)
UPLOAD_FOLDER = 'uploads'
if not os.path.exists(UPLOAD_FOLDER):
    os.makedirs(UPLOAD_FOLDER)

@app.route('/upload', methods=['POST'])
def upload_file():
    if 'file' not in request.files:
        return jsonify({"message": "No file uploaded"}), 400
    file = request.files['file']
    file_path = os.path.join(UPLOAD_FOLDER, file.filename)
    file.save(file_path)
    return jsonify({"message": "File uploaded successfully", "file_path": file_path})

# Serve Static Files (CSS, JS, Images)
@app.route('/static/<path:filename>')
def serve_static(filename):
    return send_from_directory(app.static_folder, filename)

if __name__ == '__main__':
    app.run()

